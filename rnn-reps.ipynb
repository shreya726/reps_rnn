{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.layers import LSTM, Merge\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shreya/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "df_input = pd.read_csv(\"clean_and_gaussian.csv\", index_col=0)\n",
    "df_input_segmented = pd.read_csv(\"clean_and_gaussian_segmented.csv\", index_col=0)\n",
    "df_concat = pd.concat([df_input, df_input_segmented])\n",
    "\n",
    "# Shuffle rows so that not all 10s are at the beginning and all 1s at the end\n",
    "df_concat = df_concat.sample(frac=1)\n",
    "\n",
    "y = df_concat.iloc[:,-1]\n",
    "X = df_concat.iloc[:,0:-1]\n",
    "\n",
    "X = X.fillna(0)\n",
    "y = y.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5768, 1198)\n",
      "<class 'pandas.core.frame.DataFrame'> (5768, 1198)\n",
      "<class 'pandas.core.frame.DataFrame'> (5768, 1)\n"
     ]
    }
   ],
   "source": [
    "#train test split \n",
    "dataset_size = X.shape[0]\n",
    "train_size = 8*(dataset_size//10)\n",
    "train_X = X[0:train_size]\n",
    "train_y = y[0:train_size].to_frame()\n",
    "test_X = X[train_size:]\n",
    "test_y = y[train_size:].to_frame()\n",
    "print(train_X.shape)\n",
    "print(type(train_X), train_X.shape)\n",
    "print(type(train_y), train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "#define constants\n",
    "#unrolled through 28 time steps\n",
    "time_steps=1198\n",
    "#hidden LSTM units\n",
    "num_units=25\n",
    "#rows of 28 pixels\n",
    "n_input=1\n",
    "#learning rate for adam\n",
    "learning_rate=0.001\n",
    "#mnist is meant to be classified in 11 classes(0-10).\n",
    "n_classes=11 \n",
    "#size of batch\n",
    "batch_size=25\n",
    "\n",
    "#weights and biases of appropriate shape to accomplish above task\n",
    "out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n",
    "out_bias=tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "#defining placeholders\n",
    "#input image placeholder\n",
    "x=tf.placeholder(\"float\",[None,time_steps,n_input])\n",
    "#input label placeholder\n",
    "y=tf.placeholder(\"float\",[None,n_classes])\n",
    "\n",
    "#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\n",
    "input=tf.unstack(x ,time_steps,1)\n",
    "\n",
    "#defining the network\n",
    "lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n",
    "outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "\n",
    "#loss_function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "#optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "#model evaluation\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iter  10\n",
      "Accuracy  0.12\n",
      "Loss  2.089879\n",
      "__________________\n",
      "For iter  20\n",
      "Accuracy  0.48\n",
      "Loss  1.4592088\n",
      "__________________\n",
      "For iter  30\n",
      "Accuracy  0.36\n",
      "Loss  1.560264\n",
      "__________________\n",
      "For iter  40\n",
      "Accuracy  0.52\n",
      "Loss  1.2553208\n",
      "__________________\n",
      "For iter  50\n",
      "Accuracy  0.48\n",
      "Loss  1.3102356\n",
      "__________________\n",
      "For iter  60\n",
      "Accuracy  0.56\n",
      "Loss  1.1568959\n",
      "__________________\n",
      "For iter  70\n",
      "Accuracy  0.52\n",
      "Loss  1.1893121\n",
      "__________________\n",
      "For iter  80\n",
      "Accuracy  0.44\n",
      "Loss  1.4579196\n",
      "__________________\n",
      "For iter  90\n",
      "Accuracy  0.52\n",
      "Loss  1.2492304\n",
      "__________________\n",
      "For iter  100\n",
      "Accuracy  0.52\n",
      "Loss  1.2120804\n",
      "__________________\n",
      "For iter  110\n",
      "Accuracy  0.52\n",
      "Loss  1.0509018\n",
      "__________________\n",
      "For iter  120\n",
      "Accuracy  0.48\n",
      "Loss  1.1397827\n",
      "__________________\n",
      "For iter  130\n",
      "Accuracy  0.68\n",
      "Loss  0.9139439\n",
      "__________________\n",
      "For iter  140\n",
      "Accuracy  0.36\n",
      "Loss  1.2333108\n",
      "__________________\n",
      "For iter  150\n",
      "Accuracy  0.56\n",
      "Loss  1.213009\n",
      "__________________\n",
      "For iter  160\n",
      "Accuracy  0.48\n",
      "Loss  1.2324054\n",
      "__________________\n",
      "For iter  170\n",
      "Accuracy  0.6\n",
      "Loss  1.1151989\n",
      "__________________\n",
      "For iter  180\n",
      "Accuracy  0.6\n",
      "Loss  1.098041\n",
      "__________________\n",
      "For iter  190\n",
      "Accuracy  0.56\n",
      "Loss  1.0991812\n",
      "__________________\n",
      "For iter  200\n",
      "Accuracy  0.4\n",
      "Loss  1.1182046\n",
      "__________________\n",
      "For iter  210\n",
      "Accuracy  0.52\n",
      "Loss  1.1760015\n",
      "__________________\n",
      "For iter  220\n",
      "Accuracy  0.36\n",
      "Loss  1.3192815\n",
      "__________________\n",
      "Testing Accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "#initialize variables\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    iter=1\n",
    "    start_i = 0\n",
    "    \n",
    "    while iter<230:\n",
    "        \n",
    "        train_X_temp = train_X.iloc[start_i:start_i + batch_size]\n",
    "        \n",
    "        batch_x,batch_y=train_X_temp.values.reshape(1,1198,batch_size), pd.get_dummies(train_y['reps'].values).iloc[start_i: start_i + batch_size,:]\n",
    "        \n",
    "        batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n",
    "        batch_y = batch_y.values\n",
    "        \n",
    "        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        if iter %10==0:\n",
    "            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n",
    "            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n",
    "            \n",
    "            print(\"For iter \",iter)\n",
    "            print(\"Accuracy \",acc)\n",
    "            print(\"Loss \",los)\n",
    "            print(\"__________________\")\n",
    "\n",
    "        iter=iter+1\n",
    "        start_i = start_i + batch_size\n",
    "    \n",
    "    #calculating test accuracy\n",
    "    test_x_temp = test_X.iloc[0:25].values.reshape(batch_size,time_steps,n_input)\n",
    "    \n",
    "    test_y_temp = pd.get_dummies(test_y['reps'].values).iloc[0:25,:]\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_x_temp , y: test_y_temp}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iter  10\n",
      "Accuracy  0.5234375\n",
      "Loss  1.5061315\n",
      "__________________\n",
      "For iter  20\n",
      "Accuracy  0.4765625\n",
      "Loss  1.3159087\n",
      "__________________\n",
      "For iter  30\n",
      "Accuracy  0.2734375\n",
      "Loss  1.172572\n",
      "__________________\n",
      "For iter  40\n",
      "Accuracy  0.453125\n",
      "Loss  1.1294442\n",
      "__________________\n",
      "Testing Accuracy: 0.484375\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "#define constants\n",
    "#unrolled through 28 time steps\n",
    "time_steps=1198\n",
    "#hidden LSTM units\n",
    "num_units=128\n",
    "#rows of 28 pixels\n",
    "n_input=1\n",
    "#learning rate for adam\n",
    "learning_rate=0.001\n",
    "#mnist is meant to be classified in 11 classes(0-10).\n",
    "n_classes=11 \n",
    "#size of batch\n",
    "batch_size=128\n",
    "\n",
    "#weights and biases of appropriate shape to accomplish above task\n",
    "out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n",
    "out_bias=tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "#defining placeholders\n",
    "#input image placeholder\n",
    "x=tf.placeholder(\"float\",[None,time_steps,n_input])\n",
    "#input label placeholder\n",
    "y=tf.placeholder(\"float\",[None,n_classes])\n",
    "\n",
    "#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\n",
    "input=tf.unstack(x ,time_steps,1)\n",
    "\n",
    "#defining the network\n",
    "lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n",
    "outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\n",
    "\n",
    "#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "\n",
    "#loss_function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "#optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "#model evaluation\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "#initialize variables\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    iter=1\n",
    "    start_i = 0\n",
    "    \n",
    "    while iter<45:\n",
    "        \n",
    "        train_X_temp = train_X.iloc[start_i:start_i + batch_size]\n",
    "        \n",
    "        batch_x,batch_y=train_X_temp.values.reshape(1,1198,batch_size), pd.get_dummies(train_y['reps'].values).iloc[start_i: start_i + batch_size,:]\n",
    "        \n",
    "        batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n",
    "        batch_y = batch_y.values\n",
    "        \n",
    "        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        if iter %10==0:\n",
    "            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n",
    "            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n",
    "            \n",
    "            print(\"For iter \",iter)\n",
    "            print(\"Accuracy \",acc)\n",
    "            print(\"Loss \",los)\n",
    "            print(\"__________________\")\n",
    "\n",
    "        iter=iter+1\n",
    "        start_i = start_i + batch_size\n",
    "    \n",
    "    #calculating test accuracy\n",
    "    test_x_temp = test_X.iloc[0:batch_size].values.reshape(batch_size,time_steps,n_input)\n",
    "    \n",
    "    test_y_temp = pd.get_dummies(test_y['reps'].values).iloc[0:batch_size,:]\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_x_temp , y: test_y_temp}))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
